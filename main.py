import os
import sys
import logging

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ
def check_dependencies():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∏—Ö —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å"""
    missing_deps = []
    
    try:
        import pydub
    except ImportError:
        missing_deps.append("pydub")
    
    try:
        from moviepy import VideoFileClip
    except ImportError:
        missing_deps.append("moviepy")
    
    try:
        import vosk
    except ImportError:
        missing_deps.append("vosk")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–æ–¥–µ–ª–∏ Vosk
    if not os.path.exists("vosk-model") or not os.listdir("vosk-model"):
        missing_deps.append("vosk-model")
    
    if missing_deps:
        print("‚ùå –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:")
        for dep in missing_deps:
            print(f"   - {dep}")
        print()
        print("üõ†Ô∏è  –î–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ:")
        print("   python auto_setup.py")
        print()
        print("üìö –ò–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –≤—Ä—É—á–Ω—É—é:")
        print("   pip install -r requirements.txt")
        if "vosk-model" in missing_deps:
            print("   –∏ —Å–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å Vosk –∏–∑ https://alphacephei.com/vosk/models/")
        
        sys.exit(1)

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –ø–µ—Ä–µ–¥ –∏–º–ø–æ—Ä—Ç–æ–º
check_dependencies()

import pydub
from moviepy import VideoFileClip, CompositeVideoClip, ColorClip, concatenate_videoclips, vfx
import vosk
import json


def find_silence(video, silence_threshold=-20, mn_silence_duration=750):
    file_name = 'res/audio.wav'
    video.audio.write_audiofile(file_name, logger=None)
    audio = pydub.AudioSegment.from_file(file_name)

    starts = []
    ends = []

    current_start = None

    for i in range(len(audio)):
        if audio[i].dBFS < silence_threshold and current_start is None:
            current_start = i
        elif audio[i].dBFS >= silence_threshold:
            if current_start is not None and i - current_start > mn_silence_duration:
                starts.append(current_start)
                ends.append(i)
            current_start = None

    if current_start is not None:
        starts.append(current_start)
        ends.append(len(audio))

    r = [(start / 1000, end / 1000) for start, end in zip(starts, ends)]

    if r[0][0] == 0:
        r = r[1:]

    return r


def get_non_silences(silences):
    r = []

    last = 0
    for start, end in silences:
        r.append((last, start))
        last = end

    return r


def split_to_clips(video, non_silences):
    clips = []

    for start, end in non_silences:
        dl = .3
        start -= dl if start - dl >= 0 else 0
        end += dl
        clips.append(video.subclip(start, end))

    return clips


def recognize_text(clip):
    file_name = 'res/audio_piece.wav'

    audio = clip.audio
    audio.write_audiofile(file_name, logger=None)

    audio = pydub.AudioSegment.from_file(file_name)
    audio = audio.set_frame_rate(16000).set_channels(1).set_sample_width(2)
    data = audio.raw_data

    rec.AcceptWaveform(data)
    result = rec.Result()

    return json.loads(result)['text']


def speed_up(video, clip, non_silences, i):
    start = non_silences[i][1]
    end = non_silences[i + 1][0] if i + 1 < len(clips) else None

    video_piece = video.subclip(start, end).without_audio()
    audio_piece = clip.audio

    r = video_piece.with_effects([vfx.MultiplySpeed(video_piece.duration / audio_piece.duration)])
    r = r.with_audio(audio_piece)

    return r


logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(message)s'
)

logger = logging.getLogger(__name__)


# –æ–±—ä—è–≤–ª—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
video = VideoFileClip('res/Timeline 1.mp4')
rec = vosk.KaldiRecognizer(vosk.Model('vosk-model'), 16000)


# –ø–æ–ª—É—á–∞–µ–º –∫—É—Å–æ—á–∫–∏ –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –≤–∏–¥–µ–æ
silences = find_silence(video)
non_silences = get_non_silences(silences)
clips = split_to_clips(video, non_silences)


# –≤—ã–≤–æ–¥–∏–º —Ç–µ–∫—Å—Ç –∫—É—Å–∫–æ–≤
for i, clip in enumerate(clips):
    text = recognize_text(clip)
    start, end = non_silences[i]

    print(f'{i + 1}. {start}-{end}: {text}')


# —É–¥–∞–ª–µ–Ω–∏–µ–º –ª–∏—à–Ω–µ–µ
ns = sorted(list(map(lambda x: int(x) - 1, input('–£–¥–∞–ª–∏—Ç—å: ').split())), reverse=True)
[clips.pop(i) for i in ns]
[non_silences.pop(i) for i in ns]


# —É—Å–∫–æ—Ä—è–µ–º
result = [speed_up(video, clip, non_silences, i) for i, clip in enumerate(clips)]


# —Å–æ–±–∏—Ä–∞–µ–º
video = concatenate_videoclips(result)


# —Å–æ–∑–¥–∞–µ–º —Ñ–æ–Ω
video = video.resized(1.25)
left_part = video.with_effects([vfx.Crop(x1=0, y1=0, x2=1080, y2=1440)])
background = ColorClip((1080, 1920), color=(28, 31, 32), duration=video.duration)
final_video = CompositeVideoClip([background, left_part.with_position(("center", "center"))])


final_video.write_videofile('exp.mp4')
